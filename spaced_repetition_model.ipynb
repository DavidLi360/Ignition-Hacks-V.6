{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20aaaa1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "351e022b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "from card import Card\n",
    "from constants import NEW, LEARNING, RELEARNING, REVIEW\n",
    "\n",
    "# --- Simulation Parameters ---\n",
    "WPM_BASE = 50 # Base words per minute for a simulated user\n",
    "WPM_RANGE = 10 # How much WPM can fluctuate\n",
    "\n",
    "def simulate_performance(card: Card, seconds_elapsed: int):\n",
    "    \"\"\"\n",
    "    Simulates a user's performance on a flashcard, determining if they remember it\n",
    "    and what their recall speed (WPM) would be.\n",
    "    \"\"\"\n",
    "    state = card.learning_state\n",
    "    stability = {NEW: 0.1, LEARNING: 0.3, RELEARNING: 0.5, REVIEW: 0.7}\n",
    "\n",
    "    # Probability of remembering based on forgetting curve equation\n",
    "    probability_of_remembering = math.exp(- (seconds_elapsed / (24*60*60)) / stability[state])\n",
    "\n",
    "    # Simulate WPM based on the card's state\n",
    "    if state == NEW or state == LEARNING:\n",
    "        simulated_wpm = random.uniform(WPM_BASE * 0.4, WPM_BASE * 0.8)\n",
    "    elif state == RELEARNING:\n",
    "        simulated_wpm = random.uniform(WPM_BASE * 0.7, WPM_BASE * 0.9)\n",
    "    else: \n",
    "        simulated_wpm = random.uniform(WPM_BASE * 0.8, WPM_BASE * 1.1)\n",
    "        \n",
    "    # Introduce randomness to the WPM\n",
    "    simulated_wpm += random.uniform(-WPM_RANGE, WPM_RANGE)\n",
    "    simulated_wpm = int(max(10, simulated_wpm)) # Ensure WPM doesn't go below 10\n",
    "\n",
    "    # Determine the outcome\n",
    "    remembers = random.random() < probability_of_remembering\n",
    "\n",
    "    return remembers, simulated_wpm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc50f3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    def __init__(self):\n",
    "        self.card = Card('question', 'answer', 0)\n",
    "\n",
    "    def reset(self):\n",
    "        self.card = Card('question', 'answer', 0)\n",
    "        return np.array([self.card.learn_state / 3, 0, 0])\n",
    "\n",
    "    def step(self, action):\n",
    "        # Simulate taking a step in the environment\n",
    "\n",
    "        interval_length = [\n",
    "            600,    # 10 minutes\n",
    "            86400,  # 1 day\n",
    "            259200, # 3 days\n",
    "            604800, # 1 week\n",
    "            2592000,# 1 month\n",
    "            7776000 # 3 months\n",
    "        ]\n",
    "        # action 0: review again in 10 minutes\n",
    "        remembers, wpm = simulate_performance(self.card, interval_length[action])\n",
    "\n",
    "        self.card.review(wpm, remembers)\n",
    "\n",
    "        #next state, reward, done, info\n",
    "        next_state = np.array([\n",
    "            self.card.learn_state / 3,\n",
    "            self.card.wpm / self.card.max_wpm if self.card.max_wpm > 0 else 0,\n",
    "            interval_length[action] / 7776000  # Normalize interval length to a range of 0 to 1\n",
    "        ])\n",
    "\n",
    "        reward = wpm/self.card.max_wpm if remembers else 0\n",
    "        done = False if action != 5 or not remembers or reward < 0.5 else True\n",
    "        info = None\n",
    "        return next_state, reward, done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "059ad713",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment()\n",
    "input_shape = [3]  # State space normalized WPM, learning state, \n",
    "n_outputs = 6  # Action space (10 min, 1 day, 3 days, 1 week, 1 month, 3 months)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(shape=input_shape),\n",
    "    tf.keras.layers.Dense(32, activation='elu'),\n",
    "    tf.keras.layers.Dense(32, activation='elu'),\n",
    "    tf.keras.layers.Dense(32, activation='elu'),\n",
    "    tf.keras.layers.Dense(32, activation='elu'),\n",
    "    tf.keras.layers.Dense(n_outputs)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff1f305",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_action(state, epsilon):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(0, n_outputs)\n",
    "    else:\n",
    "        Q_values = model.predict(np.array(state[np.newaxis]))\n",
    "        return np.argmax(Q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6ac2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "replay_buffer = deque(maxlen=10000)\n",
    "\n",
    "def sample_experiences(batch_size):\n",
    "    indices = np.random.randint(len(replay_buffer), size=batch_size)\n",
    "    batch = [replay_buffer[i] for i in indices]\n",
    "    states, actions, rewards, next_states, dones = map(np.array, zip(*batch))\n",
    "    return states, actions, rewards, next_states, dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274a36f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_one_step(env, state, epsilon):\n",
    "    action = epsilon_greedy_action(state, epsilon)\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    replay_buffer.append((state, action, reward, next_state, done))\n",
    "    return next_state, reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1302b0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "discount_factor = 0.99\n",
    "optimizer = tf.keras.optimizers.Adam(lr=1e-3)\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "def train_step(batch_size):\n",
    "    states, actions, rewards, next_states, dones = sample_experiences(batch_size)\n",
    "    next_Q_values = model.predict(next_states)\n",
    "    max_next_Q_values = np.max(next_Q_values, axis=1)\n",
    "    target_Q_values = rewards + (1 - dones) * discount_factor * max_next_Q_values\n",
    "\n",
    "    mask = tf.one_hot(actions, n_outputs)\n",
    "    with tf.GradientTape() as tape:\n",
    "        q_values = model(states)\n",
    "        q_values = tf.reduce_sum(q_values * mask, axis=1)\n",
    "        loss = loss_fn(target_Q_values, q_values)\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdde941",
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(500):\n",
    "    # obs = env.reset()\n",
    "    obs = env.reset()\n",
    "    for step in range(200):\n",
    "        epsilon = max(1 - episode / 500, 0.01)\n",
    "        obs, reward, done = play_one_step(env, obs, epsilon)\n",
    "        if done:\n",
    "            break\n",
    "    if episode >= 50:\n",
    "        train_step(batch_size)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
